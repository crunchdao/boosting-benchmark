# boosting-benchmark

## Notes

We are interested in benchmarking various gradient boosting routines against CrunchDAO financial datasets, both in terms of out-of-sample accuracy, robustness computation time. Work triggered by the interest of the DAO in various programming languages (Python, R, Julia), and by the existance of packages such as:

- https://xgboost.readthedocs.io/en/stable/
- https://lightgbm.readthedocs.io/en/latest/index.html
- https://github.com/Evovest/EvoTrees.jl

We can then investigate also random forest algos. From a more theorical point of view, here we are interested in understanding the virtue of complexity. On this, interesting references are [this paper](https://arxiv.org/pdf/2207.08815.pdf) and this [paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3984925)/[presentation](https://www.youtube.com/watch?v=MoMm7kSLASI).